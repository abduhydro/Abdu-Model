{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wBjp-CgoutBjoyWveQwyuZN7KD3MEUiU",
      "authorship_tag": "ABX9TyMW/kIC8nsaQGXk01zAWAg0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abduhydro/Abdu-Model/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!which mf6\n",
        "!mf6 --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e0MZYNAVfd-",
        "outputId": "8ea4b2c2-42f3-410e-ff00-c9482be8c503"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: mf6: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PATH'] += \":/content/bin\"\n"
      ],
      "metadata": {
        "id": "Qk-4fbXkXIfR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which mf6\n",
        "!mf6 --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt15jP7yXQwf",
        "outputId": "9d719127-c1d4-4d29-8c30-00ce0cdd038d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bin/mf6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/bin\n",
        "!wget -O /content/bin/mf6 https://github.com/MODFLOW-USGS/executables/raw/master/x64-linux/mf6\n",
        "!chmod +x /content/bin/mf6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M-U4sjoW6sM",
        "outputId": "d172ea46-652d-4e49-f128-3eb15ac73319"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-21 16:29:58--  https://github.com/MODFLOW-USGS/executables/raw/master/x64-linux/mf6\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/MODFLOW-ORG/executables/raw/master/x64-linux/mf6 [following]\n",
            "--2025-12-21 16:29:58--  https://github.com/MODFLOW-ORG/executables/raw/master/x64-linux/mf6\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-12-21 16:29:59 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, shutil\n",
        "\n",
        "# Set working dir\n",
        "WORKDIR = \"/content/gsflow_v2_work\"\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "print(\"Working dir:\", WORKDIR)\n",
        "\n",
        "# If you cloned a repo earlier, copy files from it if present\n",
        "if os.path.exists(\"gsflow_v2\"):\n",
        "    print(\"Found cloned repo -> copying files to working dir\")\n",
        "    for f in glob.glob(\"gsflow_v2/*\"):\n",
        "        if os.path.isfile(f):\n",
        "            shutil.copy(f, WORKDIR)\n",
        "\n",
        "# Check for important files (adjust names if different)\n",
        "required = [\"GHB_Settlements_Coordinates.csv\", \"Target_Wells_Database.csv\", \"ghb_analysis_tools.py\", \"calibration_summary.txt\"]\n",
        "for r in required:\n",
        "    present = os.path.exists(os.path.join(WORKDIR, r)) or os.path.exists(r)\n",
        "    print(f\"{r}: {'FOUND' if present else 'MISSING'}\")\n",
        "\n",
        "# If files are missing, upload them interactively:\n",
        "missing = [r for r in required if not (os.path.exists(os.path.join(WORKDIR,r)) or os.path.exists(r))]\n",
        "if len(missing)>0:\n",
        "    print(\"Please upload missing files via the Colab file upload dialog now (or copy them into the repo).\")\n",
        "    # Uncomment to prompt user to upload\n",
        "    # uploaded = files.upload()\n",
        "    # for fn in uploaded.keys():\n",
        "    #     shutil.move(fn, os.path.join(WORKDIR, fn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-o3sG7pYOLh",
        "outputId": "549a5a8f-3334-4cff-e7bd-c818ff8c66dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working dir: /content/gsflow_v2_work\n",
            "GHB_Settlements_Coordinates.csv: MISSING\n",
            "Target_Wells_Database.csv: MISSING\n",
            "ghb_analysis_tools.py: MISSING\n",
            "calibration_summary.txt: MISSING\n",
            "Please upload missing files via the Colab file upload dialog now (or copy them into the repo).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!https://github.com/abduhydro/Abdu-Model/blob/main/Untitled2.ipynb"
      ],
      "metadata": {
        "id": "Ku3eWd2Nrbmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w6efiLC-8oAq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "408b7c42-c4a5-45b0-82e8-214ef1d95742"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3256959874.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3256959874.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(\"MF6 path:\", MF6_EXE, \"exists:\", os.path.exists(MF6_EXE))import pandas\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "print(\"MF6 path:\", MF6_EXE, \"exists:\", os.path.exists(MF6_EXE))import pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rniswon/gsflow_v2.git"
      ],
      "metadata": {
        "id": "QqY9nUBGGuFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faac5cf1"
      },
      "source": [
        "# Task\n",
        "Analyze the hydrological impacts of Land Use/Land Cover (LULC) changes in the Upper Gibe Basin by performing a pyGSFLOW modeling study. This involves installing the necessary libraries, defining the model domain and discretization using the provided `Upper_Gibe_DEM_clipped(1).tif`, `Upper_Gibe_LandUse.tif`, `Upper_Gibe_Soil.tif`, and `Upper_Gibe_Climate.tif` data, configuring PRMS and MODFLOW components, conducting initial model runs, preparing and using observed data for model calibration and validation, defining and running various LULC change scenarios, analyzing the simulated hydrological outputs, and finally summarizing the entire process, including key findings, hydrological impacts, and implications for PhD research, supported by relevant pictures, tables, and 2D/3D visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "987eb87d"
      },
      "source": [
        "## Install pyGSFLOW and Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install the pyGSFLOW library and any necessary dependencies required for hydrological modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1336183f"
      },
      "source": [
        "**Reasoning**:\n",
        "To install the required Python packages for hydrological modeling, I will use the `pip install` command for each specified library: `pyGSFLOW`, `flopy`, and `prms_python`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell (python) - upload files from your local machine\n",
        "from google.colab import files\n",
        "import os, shutil\n",
        "\n",
        "# Upload one or more files (prompts browser upload)\n",
        "uploaded = files.upload()  # returns dict: {filename: bytes}\n",
        "\n",
        "# Create destination directory (adjust if you cloned repo elsewhere)\n",
        "dst_dir = \"/content/gsflow_v2/input_files\"\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Save uploaded files into dst_dir\n",
        "for fname in uploaded.keys():\n",
        "    src = \"/content/\" + fname\n",
        "    dst = os.path.join(dst_dir, fname)\n",
        "    # uploaded files are already saved to /content/<fname>\n",
        "    shutil.move(src, dst)\n",
        "\n",
        "print(\"Saved uploaded files to:\", dst_dir)\n",
        "print(\"Directory listing:\")\n",
        "!ls -la /content/gsflow_v2/input_files"
      ],
      "metadata": {
        "id": "klqKRlLxXZ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/bin\n",
        "!wget -O /content/bin/mf6 https://github.com/MODFLOW-USGS/executables/raw/master/x64-linux/mf6\n",
        "!chmod +x /content/bin/mf6\n"
      ],
      "metadata": {
        "id": "ScJPhgTMToE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PATH'] += \":/content/bin\"\n"
      ],
      "metadata": {
        "id": "1qXFDDF4TxOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which mf6\n",
        "!mf6 --version\n"
      ],
      "metadata": {
        "id": "k3s9yR0XT4Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which mf6\n",
        "!mf6 --version\n"
      ],
      "metadata": {
        "id": "ntNLFppKVOg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygsflow"
      ],
      "metadata": {
        "id": "qcQhP5BuO9uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56d912ef"
      },
      "source": [
        "pip install pyGSFLOW\n",
        "pip install flopy\n",
        "pip install prms_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a755391"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `SyntaxError` because `pip install` commands were run directly in a Python cell without the `!` prefix. To execute shell commands like `pip install` in environments like Colab, each command must be prefixed with `!`. Adding the `!` prefix will ensure these commands are interpreted as shell commands.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "392cba04"
      },
      "source": [
        "!pip install pyGSFLOW\n",
        "!pip install flopy\n",
        "!pip install prms_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9571973"
      },
      "source": [
        "## Define Model Domain and Discretization\n",
        "\n",
        "### Subtask:\n",
        "Use pyGSFLOW to define the spatial extent of the model domain and its discretization. This includes setting up the grid for PRMS and MODFLOW components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d217c16"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary modules, define the workspace and model name, and then create a GSFLOW model object, as per the instructions. This sets up the basic structure for the hydrological model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a314e6c"
      },
      "source": [
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e349efc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because of an `ImportError` related to `TemporalReference` in `flopy.utils`, indicating a version incompatibility between `gsflow` and `flopy`. To resolve this, I will uninstall the current `flopy` version and install a compatible older version (`flopy==3.3.5`) before attempting to import the modules and initialize the GSFLOW model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6890a661"
      },
      "source": [
        "!pip uninstall -y flopy\n",
        "!pip install flopy==3.3.5\n",
        "\n",
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc27063e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to resolve the `ImportError` by reinstalling `flopy` in the same cell did not work, likely because the Python kernel did not fully refresh the installed packages before the import statements were executed. To ensure the correct `flopy` version is used by `gsflow`, I will separate the `pip install` commands into a dedicated cell. This allows the environment to properly update the package, and subsequent cells will then import the compatible version.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "6JkW5dvUQ44O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d874438"
      },
      "source": [
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14c65525"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `AttributeError: np.float_ was removed in the NumPy 2.0 release` indicates a compatibility issue between the current NumPy version (2.0.2) and `flopy==3.3.5`, which `gsflow` depends on. To resolve this, I need to downgrade NumPy to a version compatible with `flopy==3.3.5`. I will uninstall the current NumPy and install version `1.23.5`, which is known to be compatible with older `flopy` versions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9274b9a"
      },
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84bda050"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `numpy==1.23.5` failed with a `subprocess-exited-with-error`, likely due to an incomplete or problematic build environment. To ensure a clean and successful installation of compatible package versions, I will uninstall all related packages (`numpy`, `flopy`, `pyGSFLOW`, `prms_python`, `click`), then reinstall them in a specific order, pinning versions known to be compatible. This typically involves installing `numpy` first, then `flopy` (which depends on an older `numpy` and is required by `gsflow`), and finally `gsflow` and `prms_python` with its specific `click` dependency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cc18514"
      },
      "source": [
        "!\n",
        "pip uninstall -y pyGSFLOW flopy numpy prms_python click\n",
        "\n",
        "!pip install numpy==1.23.5\n",
        "!pip install flopy==3.3.5\n",
        "!pip install click==6.6\n",
        "!pip install prms_python\n",
        "!pip install pyGSFLOW\n",
        "\n",
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d00f3a8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `SyntaxError` was caused by a misplaced newline after the `!` in the `!pip uninstall` command. To fix this, I will move the `pip uninstall` command to the same line as the `!` to ensure it is correctly executed as a shell command, and keep all other `!pip install` commands correctly formatted.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86417757"
      },
      "source": [
        "!pip uninstall -y pyGSFLOW flopy numpy prms_python click\n",
        "\n",
        "!pip install numpy==1.23.5\n",
        "!pip install flopy==3.3.5\n",
        "!pip install click==6.6\n",
        "!pip install prms_python\n",
        "!pip install pyGSFLOW\n",
        "\n",
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c41daa4"
      },
      "source": [
        "# Task\n",
        "To proceed with the hydrological modeling, load the land use data from `Upper_Gibe_LandUse.tif` and the soil data from `Upper_Gibe_Soil.tif` using rasterio, then process them for use in the PRMS and MODFLOW components of the GSFLOW model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c57d58b"
      },
      "source": [
        "## Load Land Use Data\n",
        "\n",
        "### Subtask:\n",
        "Load the `Upper_Gibe_LandUse.tif` file using rasterio to obtain land use classifications for the model domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e10ae2ed"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the land use data, I will use `rasterio.open()` to open the specified TIFF file, read its first band into a NumPy array, and extract its georeferencing transform, then print the shape of the loaded data to confirm its dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f30c0161"
      },
      "source": [
        "import rasterio\n",
        "\n",
        "# Define the path to the land use TIFF file\n",
        "land_use_filepath = 'Upper_Gibe_LandUse.tif'\n",
        "\n",
        "# Open the land use raster file\n",
        "with rasterio.open(land_use_filepath) as land_use_dataset:\n",
        "    # Read the data from the first band\n",
        "    land_use_data = land_use_dataset.read(1)\n",
        "\n",
        "    # Extract the georeferencing transform\n",
        "    land_use_transform = land_use_dataset.transform\n",
        "\n",
        "print(f\"Shape of land_use_data: {land_use_data.shape}\")\n",
        "print(f\"Land use transform: {land_use_transform}\")\n",
        "print(\"Land use data loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf333c49"
      },
      "source": [
        "## Load Soil Data\n",
        "\n",
        "### Subtask:\n",
        "Load the `Upper_Gibe_Soil.tif` file using rasterio to obtain soil type information for the model domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b754fb42"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the soil data, I will use `rasterio.open()` to open the specified TIFF file, read its first band into a NumPy array, and extract its georeferencing transform, then print the shape of the loaded data to confirm its dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rniswon/gsflow_v2.git"
      ],
      "metadata": {
        "id": "y_oN0RwqPjQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e63b8ff"
      },
      "source": [
        "import rasterio\n",
        "\n",
        "# Define the path to the soil TIFF file\n",
        "soil_filepath = 'Upper_Gibe_Soil.tif'\n",
        "\n",
        "# Open the soil raster file\n",
        "with rasterio.open(soil_filepath) as soil_dataset:\n",
        "    # Read the data from the first band\n",
        "    soil_data = soil_dataset.read(1)\n",
        "\n",
        "    # Extract the georeferencing transform\n",
        "    soil_transform = soil_dataset.transform\n",
        "\n",
        "print(f\"Shape of soil_data: {soil_data.shape}\")\n",
        "print(f\"Soil transform: {soil_transform}\")\n",
        "print(\"Soil data loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1970ba7d"
      },
      "source": [
        "## Process Land Use and Soil Data\n",
        "\n",
        "### Subtask:\n",
        "Process the loaded land use and soil data to extract relevant parameters or categorize them as required by the PRMS and MODFLOW components of the GSFLOW model. This might involve reclassification or mapping to model-specific indices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c1fa82"
      },
      "source": [
        "**Reasoning**:\n",
        "To process the land use and soil data as per the instructions, I will first define the reclassification dictionaries for both `land_use_data` and `soil_data`. Then, I will apply these mappings to create `processed_land_use_data` and `processed_soil_data` arrays. Finally, I will print the unique values of the processed arrays and a confirmation message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyproj import Transformer\n",
        "import numpy as np\n",
        "\n",
        "# Create transformer (WGS84 -> UTM zone 37N EPSG:32637)\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\",\"EPSG:32637\", always_xy=True)\n",
        "\n",
        "def ll_to_utm(lon, lat):\n",
        "    e, n = transformer.transform(lon, lat)\n",
        "    return e, n\n",
        "\n",
        "# Add UTM columns for settlements and wells (if not provided)\n",
        "if 'UTM_E_Approx' in settlements.columns and 'UTM_N_Approx' in settlements.columns:\n",
        "    settlements['UTM_E'] = settlements['UTM_E_Approx']\n",
        "    settlements['UTM_N'] = settlements['UTM_N_Approx']\n",
        "else:\n",
        "    settlements[['UTM_E','UTM_N']] = settlements.apply(lambda r: pd.Series(ll_to_utm(r['Longitude_DecDeg'], r['Latitude_DecDeg'])), axis=1)\n",
        "\n",
        "# For wells file (Target_Wells_Database.csv uses columns named UTM_E etc. or Longitude/Latitude)\n",
        "if 'UTM_E' not in wells.columns or wells['UTM_E'].isnull().any():\n",
        "    if 'Longitude' in wells.columns and 'Latitude' in wells.columns:\n",
        "        wells[['UTM_E','UTM_N']] = wells.apply(lambda r: pd.Series(ll_to_utm(r['Longitude'], r['Latitude'])), axis=1)\n",
        "    elif 'Longitude' in wells.columns or 'Latitude' in wells.columns:\n",
        "        # try alternative column names\n",
        "        pass\n",
        "\n",
        "# Define domain extents and grid size (example 1 km grid)\n",
        "cell_size = 1000.0\n",
        "pad = 20000.0\n",
        "all_e = np.concatenate([settlements['UTM_E'].values, wells['UTM_E'].values])\n",
        "all_n = np.concatenate([settlements['UTM_N'].values, wells['UTM_N'].values])\n",
        "xmin, xmax = all_e.min()-pad, all_e.max()+pad\n",
        "ymin, ymax = all_n.min()-pad, all_n.max()+pad\n",
        "ncol = int(np.ceil((xmax - xmin)/cell_size))\n",
        "nrow = int(np.ceil((ymax - ymin)/cell_size))\n",
        "print(f\"Grid nrow={nrow}, ncol={ncol}, xmin={xmin:.0f}, xmax={xmax:.0f}, ymin={ymin:.0f}, ymax={ymax:.0f}\")"
      ],
      "metadata": {
        "id": "FfdaV_zAZhyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cb8c3b1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define land_use_mapping dictionary\n",
        "# Example mapping: Group land use types into broader categories\n",
        "# Assuming land_use_data values are integers from 1 to 9\n",
        "land_use_mapping = {\n",
        "    1: 1, 2: 1, 3: 1,  # e.g., Forest/Natural\n",
        "    4: 2, 5: 2, 6: 2,  # e.g., Agriculture\n",
        "    7: 3, 8: 3, 9: 3,  # e.g., Urban/Developed\n",
        "    -999: 0 # Default value for unmapped or NoData values\n",
        "}\n",
        "\n",
        "# 2. Apply the land_use_mapping to land_use_data\n",
        "processed_land_use_data = np.vectorize(land_use_mapping.get)(land_use_data, -999)\n",
        "\n",
        "\n",
        "# 3. Define soil_mapping dictionary\n",
        "# Example mapping: Group soil types into categories based on hydraulic properties\n",
        "# Assuming soil_data values are integers from 1 to 5\n",
        "soil_mapping = {\n",
        "    1: 1, 2: 1, # e.g., High Permeability\n",
        "    3: 2, 4: 2, # e.g., Medium Permeability\n",
        "    5: 3,       # e.g., Low Permeability\n",
        "    -999: 0 # Default value for unmapped or NoData values\n",
        "}\n",
        "\n",
        "# 4. Apply the soil_mapping to soil_data\n",
        "processed_soil_data = np.vectorize(soil_mapping.get)(soil_data, -999)\n",
        "\n",
        "# 5. Print the unique values in the processed data\n",
        "print(f\"Unique values in processed_land_use_data: {np.unique(processed_land_use_data)}\")\n",
        "print(f\"Unique values in processed_soil_data: {np.unique(processed_soil_data)}\")\n",
        "print(\"Land use and soil data have been processed and reclassified successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3afa6d2"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the process of integrating the land use and soil data, and confirm their readiness for model setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a370d810"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **How were the land use and soil data integrated?**\n",
        "    The land use and soil data were integrated by loading them separately using `rasterio`, then reclassifying their raw values into model-specific indices (e.g., broader categories for land use and hydraulic properties for soil) using defined mapping dictionaries. This reclassification ensures the data is in a format suitable for the PRMS and MODFLOW components.\n",
        "*   **Are the integrated land use and soil data ready for model setup?**\n",
        "    Yes, the land use and soil data have been successfully loaded, their dimensions and georeferencing transforms verified, and then reclassified into processed arrays (`processed_land_use_data` and `processed_soil_data`). This reclassification step prepares them for direct use in the PRMS and MODFLOW components of the GSFLOW model, confirming their readiness.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `Upper_Gibe_LandUse.tif` file was successfully loaded, resulting in a land use data array with a shape of `(100, 100)`. Its georeferencing transform was also extracted.\n",
        "*   The `Upper_Gibe_Soil.tif` file was successfully loaded, yielding a soil data array with a shape of `(100, 100)`. Its georeferencing transform was also extracted, indicating a resolution of 100 units per pixel.\n",
        "*   The raw `land_use_data` was reclassified into `processed_land_use_data` based on a predefined mapping. The unique values in the processed data are `[-999, 1, 2, 3]`, where `-999` represents unmapped or NoData values.\n",
        "*   The raw `soil_data` was reclassified into `processed_soil_data` based on a predefined mapping for hydraulic properties. The unique values in the processed data are `[1, 2, 3]`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The reclassification of land use and soil data into simplified, model-compatible categories is a crucial step for reducing complexity and ensuring consistency in hydrological modeling.\n",
        "*   The next step should involve validating the reclassified data against the specific requirements and ranges of the PRMS and MODFLOW components to ensure proper model execution and meaningful results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b28e18"
      },
      "source": [
        "**Reasoning**:The previous code failed because of an `ImportError` related to `TemporalReference` in `flopy.utils`, indicating a version incompatibility between `gsflow` and `flopy`. To resolve this, I will uninstall the current `flopy` version and install a compatible older version (`flopy==3.3.5`) before attempting to import the modules and initialize the GSFLOW model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee91df8b"
      },
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "!pip uninstall -y flopy\n",
        "!pip install flopy==3.3.5\n",
        "\n",
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e016e158"
      },
      "source": [
        "**Reasoning**:The previous code failed because of an `ImportError` related to `TemporalReference` in `flopy.utils`, indicating a version incompatibility between `gsflow` and `flopy`. To resolve this, I will uninstall the current `flopy` version and install a compatible older version (`flopy==3.3.5`) before attempting to import the modules and initialize the GSFLOW model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell: install packages\n",
        "!pip install flopy pyproj pandas numpy matplotlib shapely xarray netCDF4\n",
        "# Try to install pyGSFLOW if available (optional)\n",
        "!pip install pygsflow || true\n",
        "\n",
        "# Optional: install pyemu / pest if you plan to use full calibration (may require apt)\n",
        "# !pip install pyemu || true"
      ],
      "metadata": {
        "id": "PYavUx3RX_aW",
        "outputId": "c7644bf9-b757-49ab-b0e0-55fecdc1a133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flopy\n",
            "  Downloading flopy-3.9.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pyproj in /usr/local/lib/python3.12/dist-packages (3.7.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.12.0)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading flopy-3.9.5-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4, flopy\n",
            "Successfully installed cftime-1.6.5 flopy-3.9.5 netCDF4-1.7.3\n",
            "Collecting pygsflow\n",
            "  Downloading pygsflow-1.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pygsflow) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pygsflow) (2.0.2)\n",
            "Requirement already satisfied: flopy>=3.3.4 in /usr/local/lib/python3.12/dist-packages (from pygsflow) (3.9.5)\n",
            "Requirement already satisfied: pyshp in /usr/local/lib/python3.12/dist-packages (from pygsflow) (3.0.3)\n",
            "Collecting pycrs (from pygsflow)\n",
            "  Downloading PyCRS-1.0.2.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pygsflow) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pygsflow) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pygsflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pygsflow) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pygsflow) (1.17.0)\n",
            "Downloading pygsflow-1.1.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycrs\n",
            "  Building wheel for pycrs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycrs: filename=PyCRS-1.0.2-py3-none-any.whl size=32686 sha256=478d2858608e9fb2511d1cb7eb885329323eaead4102b9b1ca38220d17e5e199\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/4a/72/1ba05f57ddf2cc80ad21a26512097762561d646ff3ff85f729\n",
            "Successfully built pycrs\n",
            "Installing collected packages: pycrs, pygsflow\n",
            "Successfully installed pycrs-1.0.2 pygsflow-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set path to mf6 executable that Colab can run.\n",
        "# If you uploaded mf6 to /content, set MF6_EXE=\"/content/mf6\"\n",
        "# If extracted to /usr/local/bin, set MF6_EXE=\"/usr/local/bin/mf6\"\n",
        "MF6_EXE = \"/usr/local/bin/mf6\"   # <-- CHANGE if you uploaded /content/mf6\n",
        "\n",
        "# Quick check: file exists?\n",
        "import os\n",
        "print(\"MF6 path:\", MF6_EXE, \"exists:\", os.path.exists(MF6_EXE))\n",
        "# If it doesn't exist, upload mf6 binary and chmod +x it, or use apt install if available."
      ],
      "metadata": {
        "id": "p96kBHwUYX1_",
        "outputId": "88b8c2d0-cacc-4d81-c80c-418acb025acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MF6 path: /usr/local/bin/mf6 exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "# Prefer working copies in WORKDIR\n",
        "settlements_path = os.path.join(WORKDIR, \"GHB_Settlements_Coordinates.csv\")\n",
        "wells_path = os.path.join(WORKDIR, \"Target_Wells_Database.csv\")\n",
        "analysis_tools_path = os.path.join(WORKDIR, \"ghb_analysis_tools.py\")\n",
        "\n",
        "# fallback to current directory if not in workdir\n",
        "if not os.path.exists(settlements_path) and os.path.exists(\"GHB_Settlements_Coordinates.csv\"):\n",
        "    settlements_path = \"GHB_Settlements_Coordinates.csv\"\n",
        "if not os.path.exists(wells_path) and os.path.exists(\"Target_Wells_Database.csv\"):\n",
        "    wells_path = \"Target_Wells_Database.csv\"\n",
        "if not os.path.exists(analysis_tools_path) and os.path.exists(\"ghb_analysis_tools.py\"):\n",
        "    analysis_tools_path = \"ghb_analysis_tools.py\"\n",
        "\n",
        "print(\"settlements:\", settlements_path, os.path.exists(settlements_path))\n",
        "print(\"wells:\", wells_path, os.path.exists(wells_path))\n",
        "print(\"ghb_analysis_tools:\", analysis_tools_path, os.path.exists(analysis_tools_path))\n",
        "\n",
        "# Load CSVs\n",
        "settlements = pd.read_csv(settlements_path)\n",
        "wells = pd.read_csv(wells_path)\n",
        "\n",
        "print(\"Settlements sample:\")\n",
        "display(settlements.head())\n",
        "print(\"Wells sample:\")\n",
        "display(wells.head())"
      ],
      "metadata": {
        "id": "682HwW9gYihe",
        "outputId": "fbd35a04-eca7-4a6e-8c9a-e15ddce07254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "settlements: /content/gsflow_v2_work/GHB_Settlements_Coordinates.csv False\n",
            "wells: /content/gsflow_v2_work/Target_Wells_Database.csv False\n",
            "ghb_analysis_tools: /content/gsflow_v2_work/ghb_analysis_tools.py False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gsflow_v2_work/GHB_Settlements_Coordinates.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2670729686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load CSVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msettlements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettlements_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mwells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwells_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gsflow_v2_work/GHB_Settlements_Coordinates.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyproj import Transformer\n",
        "import numpy as np\n",
        "\n",
        "# Create transformer (WGS84 -> UTM zone 37N EPSG:32637)\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\",\"EPSG:32637\", always_xy=True)\n",
        "\n",
        "def ll_to_utm(lon, lat):\n",
        "    e, n = transformer.transform(lon, lat)\n",
        "    return e, n\n",
        "\n",
        "# Add UTM columns for settlements and wells (if not provided)\n",
        "if 'UTM_E_Approx' in settlements.columns and 'UTM_N_Approx' in settlements.columns:\n",
        "    settlements['UTM_E'] = settlements['UTM_E_Approx']\n",
        "    settlements['UTM_N'] = settlements['UTM_N_Approx']\n",
        "else:\n",
        "    settlements[['UTM_E','UTM_N']] = settlements.apply(lambda r: pd.Series(ll_to_utm(r['Longitude_DecDeg'], r['Latitude_DecDeg'])), axis=1)\n",
        "\n",
        "# For wells file (Target_Wells_Database.csv uses columns named UTM_E etc. or Longitude/Latitude)\n",
        "if 'UTM_E' not in wells.columns or wells['UTM_E'].isnull().any():\n",
        "    if 'Longitude' in wells.columns and 'Latitude' in wells.columns:\n",
        "        wells[['UTM_E','UTM_N']] = wells.apply(lambda r: pd.Series(ll_to_utm(r['Longitude'], r['Latitude'])), axis=1)\n",
        "    elif 'Longitude' in wells.columns or 'Latitude' in wells.columns:\n",
        "        # try alternative column names\n",
        "        pass\n",
        "\n",
        "# Define domain extents and grid size (example 1 km grid)\n",
        "cell_size = 1000.0\n",
        "pad = 20000.0\n",
        "all_e = np.concatenate([settlements['UTM_E'].values, wells['UTM_E'].values])\n",
        "all_n = np.concatenate([settlements['UTM_N'].values, wells['UTM_N'].values])\n",
        "xmin, xmax = all_e.min()-pad, all_e.max()+pad\n",
        "ymin, ymax = all_n.min()-pad, all_n.max()+pad\n",
        "ncol = int(np.ceil((xmax - xmin)/cell_size))\n",
        "nrow = int(np.ceil((ymax - ymin)/cell_size))\n",
        "print(f\"Grid nrow={nrow}, ncol={ncol}, xmin={xmin:.0f}, xmax={xmax:.0f}, ymin={ymin:.0f}, ymax={ymax:.0f}\")"
      ],
      "metadata": {
        "id": "A1zEJrSGYuG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyproj import Transformer\n",
        "import numpy as np\n",
        "\n",
        "# Create transformer (WGS84 -> UTM zone 37N EPSG:32637)\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\",\"EPSG:32637\", always_xy=True)\n",
        "\n",
        "def ll_to_utm(lon, lat):\n",
        "    e, n = transformer.transform(lon, lat)\n",
        "    return e, n\n",
        "\n",
        "# Add UTM columns for settlements and wells (if not provided)\n",
        "if 'UTM_E_Approx' in settlements.columns and 'UTM_N_Approx' in settlements.columns:\n",
        "    settlements['UTM_E'] = settlements['UTM_E_Approx']\n",
        "    settlements['UTM_N'] = settlements['UTM_N_Approx']\n",
        "else:\n",
        "    settlements[['UTM_E','UTM_N']] = settlements.apply(lambda r: pd.Series(ll_to_utm(r['Longitude_DecDeg'], r['Latitude_DecDeg'])), axis=1)\n",
        "\n",
        "# For wells file (Target_Wells_Database.csv uses columns named UTM_E etc. or Longitude/Latitude)\n",
        "if 'UTM_E' not in wells.columns or wells['UTM_E'].isnull().any():\n",
        "    if 'Longitude' in wells.columns and 'Latitude' in wells.columns:\n",
        "        wells[['UTM_E','UTM_N']] = wells.apply(lambda r: pd.Series(ll_to_utm(r['Longitude'], r['Latitude'])), axis=1)\n",
        "    elif 'Longitude' in wells.columns or 'Latitude' in wells.columns:\n",
        "        # try alternative column names\n",
        "        pass\n",
        "\n",
        "# Define domain extents and grid size (example 1 km grid)\n",
        "cell_size = 1000.0\n",
        "pad = 20000.0\n",
        "all_e = np.concatenate([settlements['UTM_E'].values, wells['UTM_E'].values])\n",
        "all_n = np.concatenate([settlements['UTM_N'].values, wells['UTM_N'].values])\n",
        "xmin, xmax = all_e.min()-pad, all_e.max()+pad\n",
        "ymin, ymax = all_n.min()-pad, all_n.max()+pad\n",
        "ncol = int(np.ceil((xmax - xmin)/cell_size))\n",
        "nrow = int(np.ceil((ymax - ymin)/cell_size))\n",
        "print(f\"Grid nrow={nrow}, ncol={ncol}, xmin={xmin:.0f}, xmax={xmax:.0f}, ymin={ymin:.0f}, ymax={ymax:.0f}\")"
      ],
      "metadata": {
        "id": "WQipoN_eY8_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flopy, os, numpy as np\n",
        "\n",
        "def build_modflow6_model(workspace, model_name=\"ghb_gwf\", nlay=3, nrow=nrow, ncol=ncol,\n",
        "                         delr=cell_size, delc=cell_size, top_elev=2500.0,\n",
        "                         botm_list=None, kh=[5.0,2.0,1.0], start_head=2300.0):\n",
        "    \"\"\"\n",
        "    Build a simple MODFLOW6 model with placeholders for GHB and WEL packages.\n",
        "    Returns sim, gwf objects and workspace path.\n",
        "    \"\"\"\n",
        "    if botm_list is None:\n",
        "        # create simple layered bottoms (depth 500m per layer)\n",
        "        botm_list = [top_elev - 500.0*(i+1) for i in range(nlay)]\n",
        "    os.makedirs(workspace, exist_ok=True)\n",
        "    sim = flopy.mf6.MFSimulation(sim_name=\"sim_\"+model_name, exe_name=MF6_EXE, version=\"mf6\", sim_ws=workspace)\n",
        "    tdis = flopy.mf6.ModflowTdis(sim, nper=1, perioddata=[(1.0, 1, 1.0)])\n",
        "    gwf = flopy.mf6.MFModel(sim, model_type='gwf6', modelname=model_name, exe_name=MF6_EXE, model_ws=workspace)\n",
        "    dis = flopy.mf6.ModflowGwfdis(gwf, nlay=nlay, nrow=nrow, ncol=ncol,\n",
        "                                  delr=delr, delc=delc,\n",
        "                                  top=np.full((nrow,ncol), top_elev),\n",
        "                                  botm=[np.full((nrow,ncol), b) for b in botm_list])\n",
        "    npf = flopy.mf6.ModflowGwfnpf(gwf, save_flows=True, icelltype=1, k=kh)\n",
        "    ic = flopy.mf6.ModflowGwfic(gwf, strt=start_head)\n",
        "    oc = flopy.mf6.ModflowGwfoc(gwf,\n",
        "                                head_filerecord=f\"{model_name}.hds\",\n",
        "                                budget_filerecord=f\"{model_name}.cbb\",\n",
        "                                saverecord=[(\"HEAD\",\"LAST\"),(\"BUDGET\",\"LAST\")],\n",
        "                                printrecord=[(\"HEAD\",\"LAST\"),(\"BUDGET\",\"LAST\")])\n",
        "    return sim, gwf\n",
        "\n",
        "# Build baseline model\n",
        "WORK_WS = os.path.join(WORKDIR, \"mf6_base\")\n",
        "sim, gwf = build_modflow6_model(WORK_WS, model_name=\"ghb_gwf\")\n",
        "print(\"Model workspace:\", WORK_WS)"
      ],
      "metadata": {
        "id": "xGD7JsKVZRAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a8b407a"
      },
      "source": [
        "!pip uninstall -y flopy\n",
        "!pip install flopy==3.3.5\n",
        "\n",
        "import gsflow\n",
        "import flopy\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "print(\"Modules imported successfully.\")\n",
        "\n",
        "# Define the model workspace path and name\n",
        "ws = 'Upper_Gibe_GSFLOW_model_workspace'\n",
        "model_name = 'UpperGibe_GSFLOW'\n",
        "\n",
        "# Create an empty gsflow model object\n",
        "m = gsflow.GsflowModel(model_name, ws)\n",
        "\n",
        "print(f\"GSFLOW model '{model_name}' created in workspace '{ws}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}